{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepview import DeepView\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import os\n",
    "# ---------------------------\n",
    "import demo_utils as demo\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib qt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matplotlib qt seems to be a bit buggy with notebooks, so we execute it multiple times\n",
    "%matplotlib qt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load CIFAR10 and a some models\n",
    "\n",
    "This notebook tests the DeepView framework on different classifiers\n",
    "\n",
    " * ResNet-20 on CIFAR10\n",
    " * DecisionTree on MNIST\n",
    " * RandomForest on MNIST\n",
    " * KNN on MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: all CUDA-capable devices are busy or unavailable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-42a051440864>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcifar_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcifar_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdemo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_cifar_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mresnet20\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdemo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_trained_resnet20\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mdigits_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdigits_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdemo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_digit_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/luca/LocalDiskAsWell/python_projects/DeepView/DeepView/demo_utils.py\u001b[0m in \u001b[0;36mcreate_trained_resnet20\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcreate_trained_resnet20\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresnet20\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTORCH_WEIGHTS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataParallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'state_dict'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/luca/a87b7b03-b48e-49f3-903a-66c8d0de2388/python_projects/virt_envs/tf2_venv/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    424\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m'encoding'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m             \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    427\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnew_fd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/luca/a87b7b03-b48e-49f3-903a-66c8d0de2388/python_projects/virt_envs/tf2_venv/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    611\u001b[0m     \u001b[0munpickler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnpickler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m     \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersistent_load\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpersistent_load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 613\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    614\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    615\u001b[0m     \u001b[0mdeserialized_storage_keys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/luca/a87b7b03-b48e-49f3-903a-66c8d0de2388/python_projects/virt_envs/tf2_venv/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mpersistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m    574\u001b[0m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m                 \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_torch_load_uninitialized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 576\u001b[0;31m                 \u001b[0mdeserialized_objects\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mroot_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrestore_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    577\u001b[0m             \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeserialized_objects\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mroot_key\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mview_metadata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/luca/a87b7b03-b48e-49f3-903a-66c8d0de2388/python_projects/virt_envs/tf2_venv/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mrestore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    444\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mrestore_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 446\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdefault_restore_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap_location\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    447\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mrestore_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/luca/a87b7b03-b48e-49f3-903a-66c8d0de2388/python_projects/virt_envs/tf2_venv/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdefault_restore_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_package_registry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/luca/a87b7b03-b48e-49f3-903a-66c8d0de2388/python_projects/virt_envs/tf2_venv/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_cuda_deserialize\u001b[0;34m(obj, location)\u001b[0m\n\u001b[1;32m    133\u001b[0m             \u001b[0mstorage_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mstorage_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/luca/a87b7b03-b48e-49f3-903a-66c8d0de2388/python_projects/virt_envs/tf2_venv/lib/python3.6/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_new\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    632\u001b[0m     \u001b[0;31m# We need this method only for lazy init, so we can remove it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0m_CudaBase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__new__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_CudaBase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__new__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    635\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: all CUDA-capable devices are busy or unavailable"
     ]
    }
   ],
   "source": [
    "# device will be detected automatically\n",
    "# Set to 'cpu' or 'cuda:0' to set the device manually\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "cifar_X, cifar_y = demo.make_cifar_dataset()\n",
    "resnet20 = demo.create_trained_resnet20(device)\n",
    "\n",
    "digits_X, digits_y = demo.make_digit_dataset()\n",
    "decision_tree = demo.create_decision_tree(digits_X, digits_y, max_depth=10)\n",
    "random_forest = demo.create_random_forest(digits_X, digits_y, n_estimators=100)\n",
    "kn_neighbors = demo.create_kn_neighbors(digits_X, digits_y, k=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage Instructions\n",
    "\n",
    " 1. Create a wrapper funktion like ```pred_wrapper``` which receives a numpy array of samples and returns according class probabilities from the classifier as numpy arrays\n",
    " 2. Initialize DeepView-object and pass the created method to the constructor\n",
    " 3. Run your code and call ```add_samples(samples, labels)``` at any time to add samples to the visualization together with the ground truth labels.\n",
    "    * The ground truth labels will be visualized along with the predicted labels\n",
    "    * The object will keep track of a maximum number of samples specified by ```max_samples``` and it will throw away the oldest samples first\n",
    " 4. Call the ```show``` method to render the plot\n",
    "\n",
    "The following parameters must be specified on initialization:\n",
    "\n",
    "\n",
    "| <p align=\"left\">Variable    | <p align=\"left\">Meaning             |\n",
    "|----------------------|-------------------|\n",
    "| <p align=\"left\">```batch_size```    | <p align=\"left\">Batch size to use when calling the classifier |\n",
    "| <p align=\"left\">```pred_wrapper```    | <p align=\"left\">To enable DeepView to call the classifier |\n",
    "| <p align=\"left\">```max_samples```      | <p align=\"left\">The maximum amount of samples that DeepView will keep track of |\n",
    "| <p align=\"left\">```data_shape```         | <p align=\"left\">Shape of the input data (complete shape; for images, include the channel dimension) |\n",
    "| <p align=\"left\">```n```     | <p align=\"left\">Number of interpolations for distance calculation of two images. |\n",
    "| <p align=\"left\">```lam```     | <p align=\"left\">Weights the euclidian distance-component against the discriminative fisher distance. A high lambda puts emphasis on the euclidian distance, so structural information of the data will be more relevant for distance computation. |\n",
    "| <p align=\"left\">```resolution```       | <p align=\"left\">x- and y- Resolution of the decision boundary plot |\n",
    "| <p align=\"left\">```cmap```             | <p align=\"left\">Name of the colormap that should be used in the plots. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo with Torch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# softmax operation to use in pred_wrapper\n",
    "softmax = torch.nn.Softmax(dim=-1)\n",
    "\n",
    "# this is the prediction wrapper, it encapsulates the call to the model\n",
    "# and does all the casting to the appropriate datatypes\n",
    "def pred_wrapper(x):\n",
    "    with torch.no_grad():\n",
    "        x = np.array(x, dtype=np.float32)\n",
    "        x = np.transpose(x, [0, 3, 1, 2])\n",
    "        tensor = torch.from_numpy(x).to(device)\n",
    "        logits = resnet20(tensor)\n",
    "        probabilities = softmax(logits).cpu().numpy()\n",
    "    return probabilities\n",
    "\n",
    "# the classes in the dataset to be used as labels in the plots\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "# --- Deep View Parameters ----\n",
    "batch_size = 1024\n",
    "max_samples = 500\n",
    "data_shape = (32, 32, 3)\n",
    "n = 3\n",
    "lam = .64\n",
    "resolution = 100\n",
    "cmap = 'tab10'\n",
    "title = 'ResNet-20 - CIFAR10'\n",
    "\n",
    "deepview = DeepView(pred_wrapper, classes, max_samples, batch_size, \n",
    "                    data_shape, n, lam, resolution, cmap, title=title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to calculate visualization for 150 samples: 29.42 sec\n"
     ]
    }
   ],
   "source": [
    "n_samples = 150\n",
    "sample_ids = np.random.choice(len(cifar_X), n_samples)\n",
    "X = np.array([ cifar_X[i] for i in sample_ids ])\n",
    "Y = np.array([ cifar_y[i] for i in sample_ids ])\n",
    "\n",
    "t0 = time.time()\n",
    "deepview.add_samples(X, Y)\n",
    "deepview.show()\n",
    "\n",
    "\n",
    "print('Time to calculate visualization for %d samples: %.2f sec' % (n_samples, time.time() - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add new samples to the visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance calculation 20.00 %\n",
      "Distance calculation 40.00 %\n",
      "Distance calculation 60.00 %\n",
      "Distance calculation 80.00 %\n",
      "Distance calculation 100.00 %\n",
      "Embedding samples ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luca/Downloads/umap/umap/umap_.py:1428: UserWarning: Using precomputed metric; transform will be unavailable for new data\n",
      "  warn(\"Using precomputed metric; transform will be unavailable for new data\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing decision regions ...\n",
      "Time to add 200 samples to visualization: 82.53 sec\n"
     ]
    }
   ],
   "source": [
    "n_new = 200\n",
    "\n",
    "sample_ids = np.random.choice(len(cifar_X), n_new)\n",
    "X = np.array([ cifar_X[i] for i in sample_ids ])\n",
    "Y = np.array([ cifar_y[i] for i in sample_ids ])\n",
    "\n",
    "t0 = time.time()\n",
    "deepview.add_samples(X, Y)\n",
    "deepview.show()\n",
    "\n",
    "print('Time to add %d samples to visualization: %.2f sec' % (n_new, time.time() - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example output\n",
    "\n",
    "As the plot is updatable, it is shown in a separate Qt-window. With the CIFAR-data and the model loaded above, the following plot was produced after 200 samples where added:\n",
    "\n",
    "**Hyperparameters:**\n",
    "n = 10\n",
    "lam = 0.2\n",
    "resolution = 100\n",
    "\n",
    "![sample_plot](https://user-images.githubusercontent.com/30961397/72370639-fbab6f00-3702-11ea-98f4-0dc7335777fc.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning the $\\lambda$-Hyperparameter\n",
    "\n",
    "> The $\\lambda$-Hyperparameter weights the euclidian distance component.\n",
    "> When the visualization doesn't show class-clusters, **try a smaller lambda** to put more emphasis on the discriminative distance component that considers the class.\n",
    "> A smaller $\\lambda$ will pull the datapoints further into their class-clusters.\n",
    "> Therefore, a **too small $\\lambda$** can lead to collapsed clusters that don't represent any structural properties of the datapoints. Of course this behaviour also depends on the data and how well the label corresponds to certain structural properties.\n",
    "\n",
    "Due to separate handling of euclidian and class-discriminative distances, the $\\lambda$ parameter can easily be adjusted. Distances don't need to be recomputed, only the embeddings and therefore also the plot of the decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding samples ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luca/Downloads/umap/umap/umap_.py:1428: UserWarning: Using precomputed metric; transform will be unavailable for new data\n",
      "  warn(\"Using precomputed metric; transform will be unavailable for new data\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing decision regions ...\n"
     ]
    }
   ],
   "source": [
    "deepview.set_lambda(.7)\n",
    "deepview.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare performance\n",
    "\n",
    "For this test, DeepView was run on a GPU (GTX 1060 6GB).\n",
    "Adding samples may be a bit more time consuming, then just running DeepView on the desired amount of samples to be visualized. This is because the decision boundaries must be calculated twice with a similar time complexity. However, the step of adding 100 samples to 100 existing samples takes less time then computing it from scratch for 200 samples. This is because distances were already computed for half of the samples and can be reused.\n",
    "\n",
    "| <p align=\"left\">Szenario | Time |\n",
    "| -------- | ---- |\n",
    "| <p align=\"left\">From scratch for 100 samples | 31.20 sec |\n",
    "| <p align=\"left\">Adding 100 samples (100 already added) | 66.89 sec |\n",
    "| <p align=\"left\">From scratch for 200 samples | 71.16 sec |\n",
    "| <p align=\"left\">200 samples when adding 100 samples in two steps | 98.19 sec |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cifar_X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-3c9481e310e1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0msample_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcifar_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m \u001b[0mcifar_X\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msample_ids\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m \u001b[0mcifar_y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msample_ids\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cifar_X' is not defined"
     ]
    }
   ],
   "source": [
    "deepview.reset()\n",
    "\n",
    "n_samples = 200\n",
    "sample_ids = np.random.choice(len(cifar_X), n_samples)\n",
    "X = np.array([ cifar_X[i] for i in sample_ids ])\n",
    "Y = np.array([ cifar_y[i] for i in sample_ids ])\n",
    "\n",
    "t0 = time.time()\n",
    "deepview.add_samples(X, Y)\n",
    "deepview.show()\n",
    "\n",
    "print('Time to calculate visualization for %d samples: %.2f sec' % (n_samples, time.time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "deepview.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using DeepView in a training routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "mnist_ds = demo.create_torch_dataset(batch_size, train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "umap.umap_.UMAP"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# os.mkdir('results')\n",
    "import umap\n",
    "\n",
    "umap.UMAP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = demo.TorchModel()\n",
    "model = model.to(device)\n",
    "step = 0\n",
    "epochs = 2\n",
    "batches = np.ceil(len(mnist_ds)/batch_size)\n",
    "# ----------- SETUP DeewView ----------------\n",
    "title = 'ResNet-20 - CIFAR10'\n",
    "max_samples = 200\n",
    "pred_wrapper = demo.create_torch_wrapper(model, device)\n",
    "verbose = False\n",
    "classes = range(10)\n",
    "data_shape = (28, 28, 1)\n",
    "\n",
    "deepview = DeepView(pred_wrapper, classes, max_samples, batch_size, data_shape, \n",
    "                    n, lam, resolution, cmap, title=title, verbose=verbose, seed=42)\n",
    "\n",
    "# add 4 batches of points to DeepView\n",
    "ds = iter(mnist_ds)\n",
    "for i in range(4):\n",
    "    xs, ys = next(ds)\n",
    "    xs = xs.transpose(3,1).transpose(2,1).cpu()\n",
    "    deepview.add_samples(xs.numpy(), ys.cpu().numpy(), False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "deepview.lam = 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3025665\n",
      "2.2923768\n",
      "2.1530974\n",
      "2.0211568\n",
      "1.8018416\n",
      "1.7136234\n",
      "1.6662513\n",
      "1.6412671\n",
      "1.582039\n",
      "1.6288692\n",
      "1.6654189\n",
      "1.6106936\n",
      "1.6682903\n",
      "1.630942\n",
      "1.5531472\n",
      "1.5893736\n",
      "1.6786302\n",
      "1.6214026\n",
      "1.6534883\n",
      "1.5533822\n",
      "1.5617626\n",
      "1.5793964\n",
      "1.5829853\n",
      "1.5983434\n",
      "1.5686294\n",
      "1.585761\n",
      "1.5563494\n",
      "1.5375633\n",
      "1.6427846\n",
      "1.5694093\n",
      "1.511967\n",
      "1.5416645\n",
      "1.5833257\n",
      "1.5283006\n",
      "1.5613956\n",
      "1.5878615\n",
      "1.5261532\n",
      "1.5190861\n",
      "1.5218796\n",
      "1.5560772\n",
      "1.539955\n",
      "1.5708776\n",
      "1.6229061\n",
      "1.5614201\n",
      "1.6225318\n",
      "1.5302511\n",
      "1.5158979\n",
      "1.4761128\n",
      "1.5084955\n",
      "1.5335644\n",
      "1.5551655\n",
      "1.5447632\n",
      "1.488345\n",
      "1.5116361\n",
      "1.4964716\n",
      "1.6167387\n",
      "1.5570909\n",
      "1.5491014\n",
      "1.6043891\n",
      "1.5163338\n",
      "1.5476564\n",
      "1.5622323\n",
      "1.5220479\n",
      "1.5339631\n",
      "1.504228\n",
      "1.5358031\n",
      "1.5278969\n",
      "1.5016149\n",
      "1.5216564\n",
      "1.5175843\n",
      "1.5115942\n",
      "1.4951671\n",
      "1.5148455\n",
      "1.5200933\n",
      "1.5549532\n",
      "1.5814438\n",
      "1.5024128\n",
      "1.504522\n",
      "1.5372555\n",
      "1.50006\n",
      "1.4961711\n",
      "1.4930007\n",
      "1.5415187\n",
      "1.5301611\n",
      "1.5267262\n",
      "1.4915701\n",
      "1.5503973\n",
      "1.5046484\n",
      "1.4890081\n",
      "1.4960154\n",
      "1.5343136\n",
      "1.4859928\n",
      "1.4970052\n",
      "1.5481327\n",
      "1.5186193\n",
      "1.4929318\n",
      "1.5367404\n",
      "1.4923218\n",
      "1.4664534\n",
      "1.5321945\n",
      "1.5046794\n",
      "1.4678779\n",
      "1.5260713\n",
      "1.5623404\n",
      "1.5139109\n",
      "1.4955456\n",
      "1.5285513\n",
      "1.5502206\n",
      "1.5231692\n",
      "1.4771631\n",
      "1.4656467\n",
      "1.49624\n",
      "1.592897\n",
      "1.5192201\n",
      "1.4908892\n",
      "1.4816825\n",
      "1.4751463\n",
      "1.4938979\n",
      "1.4791902\n",
      "1.4926342\n",
      "1.5242804\n",
      "1.4739996\n",
      "1.4924564\n",
      "1.5244398\n",
      "1.4795558\n",
      "1.5001092\n",
      "1.4785074\n",
      "1.4963703\n",
      "1.5121871\n",
      "1.5021803\n",
      "1.5131935\n",
      "1.5085644\n",
      "1.4957515\n",
      "1.5294439\n",
      "1.4767675\n",
      "1.5243711\n",
      "1.5058402\n",
      "1.5027452\n",
      "1.5226246\n",
      "1.483062\n",
      "1.4861394\n",
      "1.4804047\n",
      "1.4796594\n",
      "1.513159\n",
      "1.4621094\n",
      "1.5638585\n",
      "1.5153348\n",
      "1.5002512\n",
      "1.5386019\n",
      "1.4906043\n",
      "1.4954575\n",
      "1.5152599\n",
      "1.5137028\n",
      "1.5252779\n",
      "1.4711261\n",
      "1.5173286\n",
      "1.5077215\n",
      "1.4944875\n",
      "1.482154\n",
      "1.4674052\n",
      "1.5072861\n",
      "1.5954328\n",
      "1.5106916\n",
      "1.4802575\n",
      "1.4764794\n",
      "1.5242642\n",
      "1.5020326\n",
      "1.5119092\n",
      "1.4770552\n",
      "1.4616979\n",
      "1.505155\n",
      "1.5565894\n",
      "1.4828869\n",
      "1.5613047\n",
      "1.4926379\n",
      "1.4619107\n",
      "1.473351\n",
      "1.5124539\n",
      "1.4952701\n",
      "1.5070431\n",
      "1.4816862\n",
      "1.4959846\n",
      "1.5061554\n",
      "1.4787809\n",
      "1.5140626\n",
      "1.498414\n",
      "1.5216566\n",
      "1.4919717\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    \n",
    "    for i, (X, y) in enumerate(mnist_ds):\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        model.optimizer.zero_grad()\n",
    "        logits = model.forward(X)\n",
    "        preds = softmax(logits)\n",
    "        loss = model.loss_fn(preds, y)\n",
    "        loss.backward()\n",
    "        model.optimizer.step()\n",
    "        step += 1\n",
    "        \n",
    "        if i % 10 == 0:\n",
    "            # update samples in deepview\n",
    "            deepview.update()\n",
    "            title = 'cifar10 step %d loss %.2f' % (step, loss.cpu().detach().numpy())\n",
    "            deepview.ax.set_title(title)\n",
    "            deepview.fig.savefig('results/deepview_mnist_%d.pdf' % step)\n",
    "            print(loss.cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-> new -> old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_ebd = deepview.embedded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ebd = deepview.embedded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.909006    2.233761  ]\n",
      " [ 2.2817965   0.01644166]\n",
      " [-0.8881552   1.1460644 ]\n",
      " [-0.24402298  3.876659  ]\n",
      " [-1.3976914   1.9913878 ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-2.1311893 ,  2.455944  ],\n",
       "       [ 2.368148  ,  0.10293511],\n",
       "       [-0.7888781 ,  1.0468191 ],\n",
       "       [-0.20028041,  3.8314996 ],\n",
       "       [-1.3965527 ,  2.0034225 ]], dtype=float32)"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(new_ebd[:5])\n",
    "old_ebd[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_min = old_ebd.min(0)\n",
    "x_max = old_ebd.max(0)\n",
    "av_range = np.mean(x_max - x_min)\n",
    "\n",
    "a = 500/av_range\n",
    "b = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "deepview.mapper.random_state = np.random.randint(1000)\n",
    "deepview.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "new_eb = umap.umap_.optimize_layout_euclidean(\n",
    "    new_ebd.copy(), old_ebd.copy(), list(range(deepview.num_samples)), list(range(deepview.num_samples)), \n",
    "    100, deepview.num_samples, np.array([1]*deepview.num_samples), a, b, np.array([1,1,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.3052449  8.849721 ]\n",
      " [ 1.1837322  8.686272 ]\n",
      " [ 2.199946   6.4908895]\n",
      " [ 3.6811585  4.8167205]\n",
      " [ 1.5350261  4.4847336]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-3.9419956, 12.466264 ],\n",
       "       [ 2.771194 ,  7.8540854],\n",
       "       [ 3.297338 ,  7.233462 ],\n",
       "       [ 5.372856 ,  4.2202163],\n",
       "       [ 2.0815883,  4.518349 ]], dtype=float32)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(new_eb[:5])\n",
    "old_ebd[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7fea0c340a20>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(2, 2)\n",
    "\n",
    "ax[0,0].scatter(new_eb)\n",
    "ax[0,1].scatter(new_eb)\n",
    "ax[1,0].scatter(new_eb)\n",
    "ax[1,1].scatter(new_eb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.0 1\n",
      "1 0.6931471805599453 2\n",
      "2 1.0986122886681098 4\n",
      "3 1.3862943611198906 8\n",
      "4 1.6094379124341003 16\n",
      "5 1.791759469228055 32\n",
      "6 1.9459101490553132 64\n",
      "7 2.0794415416798357 128\n",
      "8 2.1972245773362196 256\n",
      "9 2.302585092994046 512\n",
      "10 2.3978952727983707 1024\n",
      "11 2.4849066497880004 2048\n",
      "12 2.5649493574615367 4096\n",
      "13 2.6390573296152584 8192\n",
      "14 2.70805020110221 16384\n",
      "15 2.772588722239781 32768\n",
      "16 2.833213344056216 65536\n",
      "17 2.8903717578961645 131072\n",
      "18 2.9444389791664403 262144\n",
      "19 2.995732273553991 524288\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    #if int((2)**i % 20) == 0:\n",
    "    #    print(i)\n",
    "    print(i, np.log(i+1), int(2**i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo with RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance calculation 20.00 %\n",
      "Distance calculation 40.00 %\n",
      "Distance calculation 60.00 %\n",
      "Distance calculation 80.00 %\n",
      "Distance calculation 100.00 %\n",
      "Embedding samples ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luca/Downloads/umap/umap/umap_.py:1428: UserWarning: Using precomputed metric; transform will be unavailable for new data\n",
      "  warn(\"Using precomputed metric; transform will be unavailable for new data\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing decision regions ...\n",
      "Time to calculate visualization for 200 samples: 65.11 sec\n"
     ]
    }
   ],
   "source": [
    "pred_wrapper = DeepView.create_simple_wrapper(random_forest.predict_proba)\n",
    "\n",
    "# the digit dataset is used, so classes are [0..9]\n",
    "classes = np.arange(10)\n",
    "\n",
    "# --- Deep View Parameters ----\n",
    "batch_size = 64\n",
    "max_samples = 500\n",
    "sample_shape = (64,)\n",
    "n = 10\n",
    "lam = 0.5\n",
    "resolution = 100\n",
    "cmap = 'tab10'\n",
    "title = 'RandomForest - MNIST'\n",
    "\n",
    "# create DeepView object\n",
    "deepview = DeepView(pred_wrapper, classes, max_samples, batch_size, \n",
    "                    sample_shape, n, lam, resolution, cmap, title=title)\n",
    "\n",
    "# add data samples\n",
    "n_samples = 200\n",
    "sample_ids = np.random.choice(len(digits_X), n_samples)\n",
    "X = np.array([ digits_X[i] for i in sample_ids ])\n",
    "Y = np.array([ digits_y[i] for i in sample_ids ])\n",
    "\n",
    "t0 = time.time()\n",
    "deepview.add_samples(X, Y)\n",
    "deepview.show()\n",
    "\n",
    "print('Time to calculate visualization for %d samples: %.2f sec' % (n_samples, time.time() - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![random_forest](https://user-images.githubusercontent.com/30961397/78502477-a6ab5200-7761-11ea-8be3-e0b4c8e6a966.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "deepview.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo with DecisionTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Deep View Parameters ----\n",
    "batch_size = 256\n",
    "max_samples = 500\n",
    "# the data can also be represented as a vector\n",
    "sample_shape = (64,)\n",
    "n = 10\n",
    "lam = 0.65\n",
    "resolution = 100\n",
    "cmap = 'gist_ncar'\n",
    "\n",
    "# the digit dataset is used, so classes are [0..9]\n",
    "classes = np.arange(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance calculation 20.00 %\n",
      "Distance calculation 40.00 %\n",
      "Distance calculation 60.00 %\n",
      "Distance calculation 80.00 %\n",
      "Distance calculation 100.00 %\n",
      "Embedding samples ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luca/Downloads/umap/umap/umap_.py:1428: UserWarning: Using precomputed metric; transform will be unavailable for new data\n",
      "  warn(\"Using precomputed metric; transform will be unavailable for new data\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing decision regions ...\n",
      "Time to calculate visualization for 200 samples: 77.18 sec\n"
     ]
    }
   ],
   "source": [
    "pred_wrapper = DeepView.create_simple_wrapper(kn_neighbors.predict_proba)\n",
    "\n",
    "# create DeepView object\n",
    "deepview = DeepView(pred_wrapper, classes, max_samples, batch_size, \n",
    "                    sample_shape, n, lam, resolution, cmap)\n",
    "\n",
    "# add data samples\n",
    "n_samples = 200\n",
    "sample_ids = np.random.choice(len(digits_X), n_samples)\n",
    "X = np.array([ digits_X[i] for i in sample_ids ])\n",
    "Y = np.array([ digits_y[i] for i in sample_ids ])\n",
    "\n",
    "t0 = time.time()\n",
    "deepview.add_samples(X, Y)\n",
    "deepview.show()\n",
    "\n",
    "print('Time to calculate visualization for %d samples: %.2f sec' % (n_samples, time.time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding samples ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luca/Downloads/umap/umap/umap_.py:1428: UserWarning: Using precomputed metric; transform will be unavailable for new data\n",
      "  warn(\"Using precomputed metric; transform will be unavailable for new data\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing decision regions ...\n"
     ]
    }
   ],
   "source": [
    "deepview.set_lambda(.4)\n",
    "deepview.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "deepview.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo: KNN-Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance calculation 20.00 %\n",
      "Distance calculation 40.00 %\n",
      "Distance calculation 60.00 %\n",
      "Distance calculation 80.00 %\n",
      "Distance calculation 100.00 %\n",
      "Embedding samples ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luca/Downloads/umap/umap/umap_.py:1428: UserWarning: Using precomputed metric; transform will be unavailable for new data\n",
      "  warn(\"Using precomputed metric; transform will be unavailable for new data\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing decision regions ...\n",
      "Time to calculate visualization for 200 samples: 79.15 sec\n"
     ]
    }
   ],
   "source": [
    "pred_wrapper = DeepView.create_simple_wrapper(kn_neighbors.predict_proba)\n",
    "\n",
    "# create DeepView object\n",
    "deepview = DeepView(pred_wrapper, classes, max_samples, batch_size, \n",
    "                    sample_shape, n, lam, resolution, cmap)\n",
    "\n",
    "# add data samples\n",
    "n_samples = 200\n",
    "sample_ids = np.random.choice(len(digits_X), n_samples)\n",
    "X = np.array([ digits_X[i] for i in sample_ids ])\n",
    "Y = np.array([ digits_y[i] for i in sample_ids ])\n",
    "\n",
    "t0 = time.time()\n",
    "deepview.add_samples(X, Y)\n",
    "deepview.show()\n",
    "\n",
    "print('Time to calculate visualization for %d samples: %.2f sec' % (n_samples, time.time() - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![knn](https://user-images.githubusercontent.com/30961397/78502740-dc046f80-7762-11ea-82cf-efc8251539db.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating DeepView"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation of DeepView: ResNet-20 - CIFAR10\n",
      "\n",
      "orig labs, knn err: eucl / fish 0.29 / 0.46\n",
      "orig labs, knn err in proj space: eucl / fish 0.8 / 0.29\n",
      "classif labs, knn err: eucl / fish 0.26 / 0.5\n",
      "classif labs, knn acc in proj space: eucl / fish 17.0 / 74.0\n"
     ]
    }
   ],
   "source": [
    "from deepview.evaluate import evaluate\n",
    "\n",
    "print('Evaluation of DeepView: %s\\n' % deepview.title)\n",
    "evaluate(deepview, X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deepview.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2.0",
   "language": "python",
   "name": "tf2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
