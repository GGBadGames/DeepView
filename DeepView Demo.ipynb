{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DeepView import DeepView\n",
    "import numpy as np\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from models.torch_model import TorchModel\n",
    "import models.resnet as resnet\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib qt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load CIFAR10 and a torch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Loaded model:\t ResNet\n",
      "Best Test prec:\t 91.78000183105469\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n",
    "trainset = datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "model = resnet.resnet20()\n",
    "weights = torch.load(\"models/pytorch_resnet_cifar10-master/pretrained_models/resnet20-12fca82f.th\", \n",
    "                     map_location='cpu')\n",
    "\n",
    "model = nn.DataParallel(model)\n",
    "model.load_state_dict(weights['state_dict'])\n",
    "model = model.module\n",
    "model.eval()\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print('Loaded model:\\t', model._get_name())\n",
    "print(\"Best Test prec:\\t\", weights['best_prec1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage Instructions\n",
    "\n",
    " 1. Create a wrapper funktion like ```pred_wrapper``` which receives a numpy array of samples and returns according class probabilities from the classifier as numpy arrays\n",
    " 2. Initialize DeepView-object and pass the created method to the constructor\n",
    " 3. Run your code and call ```add_samples(samples, labels)``` at any time to add samples to the visualization together with the ground truth labels.\n",
    "    * The ground truth labels will be visualized along with the predicted labels\n",
    "    * The object will keep track of a maximum number of samples specified by ```max_samples``` and it will throw away the oldest samples first\n",
    " 4. Call the ```show``` method to render the plot\n",
    "\n",
    "The following parameters must be specified on initialization:\n",
    "\n",
    "\n",
    "| <p align=\"left\">Variable    | <p align=\"left\">Meaning             |\n",
    "|----------------------|-------------------|\n",
    "| <p align=\"left\">```pred_wrapper```    | <p align=\"left\">To enable DeepView to call the classifier |\n",
    "| <p align=\"left\">```max_samples```      | <p align=\"left\">The maximum amount of samples that DeepView will keep track of |\n",
    "| <p align=\"left\">```img_size```         | <p align=\"left\">Currently only images are supported as inputs, img size specifies width and height of the input samples |\n",
    "| <p align=\"left\">```img_channels```     | <p align=\"left\">Number of image channels |\n",
    "| <p align=\"left\">```resolution```       | <p align=\"left\">x- and y- Resolution of the decision boundary plot |\n",
    "| <p align=\"left\">```cmap```             | <p align=\"left\">Name of the colormap that should be used in the plots. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_wrapper(x):\n",
    "    with torch.no_grad():\n",
    "        x = np.array(x, dtype=np.float32)\n",
    "        tensor = torch.from_numpy(x).to(device)\n",
    "        pred = model(tensor).cpu().numpy()\n",
    "    return pred\n",
    "\n",
    "\n",
    "# --- Deep View Parameters ----\n",
    "batch_size = 32\n",
    "max_samples = 500\n",
    "img_size = 32\n",
    "img_channels = 3\n",
    "resolution = 200\n",
    "cmap = 'tab10'\n",
    "\n",
    "deepview = DeepView(pred_wrapper, classes, max_samples, batch_size, \n",
    "                    img_size, img_channels, resolution, cmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance calculation 20.00 %\n",
      "Distance calculation 40.00 %\n",
      "Distance calculation 60.00 %\n",
      "Distance calculation 80.00 %\n",
      "Distance calculation 100.00 %\n",
      "Embedding samples ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luca/Downloads/umap/umap/umap_.py:1428: UserWarning: Using precomputed metric; transform will be unavailable for new data\n",
      "  warn(\"Using precomputed metric; transform will be unavailable for new data\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing decision regions ...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "151.75577116012573"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_samples = 100\n",
    "sample_ids = np.random.choice(len(trainset), n_samples)\n",
    "X = np.array([ trainset[i][0].cpu().numpy() for i in sample_ids ])\n",
    "Y = np.array([ trainset[i][1] for i in sample_ids ])\n",
    "\n",
    "import time\n",
    "t0 = time.time()\n",
    "deepview.add_samples(X, Y)\n",
    "deepview.show()\n",
    "time.time() -t0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add new samples to the visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance calculation 20.00 %\n",
      "Distance calculation 40.00 %\n",
      "Distance calculation 60.00 %\n",
      "Distance calculation 80.00 %\n",
      "Distance calculation 100.00 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luca/Downloads/umap/umap/umap_.py:1428: UserWarning: Using precomputed metric; transform will be unavailable for new data\n",
      "  warn(\"Using precomputed metric; transform will be unavailable for new data\")\n"
     ]
    }
   ],
   "source": [
    "n_new = 100\n",
    "\n",
    "sample_ids = np.random.choice(len(trainset), n_new)\n",
    "X = np.array([ trainset[i][0].cpu().numpy() for i in sample_ids ])\n",
    "Y = np.array([ trainset[i][1] for i in sample_ids ])\n",
    "\n",
    "deepview.add_samples(X, Y)\n",
    "deepview.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2.0",
   "language": "python",
   "name": "tf2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
