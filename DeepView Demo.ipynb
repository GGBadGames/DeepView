{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepview import DeepView\n",
    "import numpy as np\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from models.torch_model import TorchModel\n",
    "import models.resnet as resnet\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "import time\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib qt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load CIFAR10 and a torch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Loaded model:\t ResNet\n",
      "Best Test prec:\t 91.78000183105469\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n",
    "trainset = datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "model = resnet.resnet20()\n",
    "weights = torch.load(\"models/pytorch_resnet_cifar10-master/pretrained_models/resnet20-12fca82f.th\", \n",
    "                     map_location='cpu')\n",
    "\n",
    "model = nn.DataParallel(model)\n",
    "model.load_state_dict(weights['state_dict'])\n",
    "model = model.module\n",
    "model.eval()\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print('Loaded model:\\t', model._get_name())\n",
    "print(\"Best Test prec:\\t\", weights['best_prec1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage Instructions\n",
    "\n",
    " 1. Create a wrapper funktion like ```pred_wrapper``` which receives a numpy array of samples and returns according class probabilities from the classifier as numpy arrays\n",
    " 2. Initialize DeepView-object and pass the created method to the constructor\n",
    " 3. Run your code and call ```add_samples(samples, labels)``` at any time to add samples to the visualization together with the ground truth labels.\n",
    "    * The ground truth labels will be visualized along with the predicted labels\n",
    "    * The object will keep track of a maximum number of samples specified by ```max_samples``` and it will throw away the oldest samples first\n",
    " 4. Call the ```show``` method to render the plot\n",
    "\n",
    "The following parameters must be specified on initialization:\n",
    "\n",
    "\n",
    "| <p align=\"left\">Variable    | <p align=\"left\">Meaning             |\n",
    "|----------------------|-------------------|\n",
    "| <p align=\"left\">```batch_size```    | <p align=\"left\">Batch size to use when calling the classifier |\n",
    "| <p align=\"left\">```pred_wrapper```    | <p align=\"left\">To enable DeepView to call the classifier |\n",
    "| <p align=\"left\">```max_samples```      | <p align=\"left\">The maximum amount of samples that DeepView will keep track of |\n",
    "| <p align=\"left\">```img_size```         | <p align=\"left\">Currently only images are supported as inputs, img size specifies width and height of the input samples |\n",
    "| <p align=\"left\">```img_channels```     | <p align=\"left\">Number of image channels |\n",
    "| <p align=\"left\">```n```     | <p align=\"left\">Number of interpolations for distance calculation of two images. |\n",
    "| <p align=\"left\">```lam```     | <p align=\"left\">Weighting factor for the euclidian component of the distance calculation. |\n",
    "| <p align=\"left\">```resolution```       | <p align=\"left\">x- and y- Resolution of the decision boundary plot |\n",
    "| <p align=\"left\">```cmap```             | <p align=\"left\">Name of the colormap that should be used in the plots. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_wrapper(x):\n",
    "    with torch.no_grad():\n",
    "        x = np.array(x, dtype=np.float32)\n",
    "        tensor = torch.from_numpy(x).to(device)\n",
    "        pred = model(tensor).cpu().numpy()\n",
    "    return pred\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "# --- Deep View Parameters ----\n",
    "batch_size = 32\n",
    "max_samples = 500\n",
    "img_size = (32, 32)\n",
    "img_channels = 3\n",
    "n = 10\n",
    "lam = 0.0001\n",
    "resolution = 100\n",
    "cmap = 'tab10'\n",
    "\n",
    "\n",
    "deepview = DeepView(pred_wrapper, classes, max_samples, batch_size, \n",
    "                    img_size, img_channels, n, lam, resolution, cmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance calculation 20.00 %\n",
      "Distance calculation 40.00 %\n",
      "Distance calculation 60.00 %\n",
      "Distance calculation 80.00 %\n",
      "Distance calculation 100.00 %\n",
      "Embedding samples ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luca/Downloads/umap/umap/umap_.py:1428: UserWarning: Using precomputed metric; transform will be unavailable for new data\n",
      "  warn(\"Using precomputed metric; transform will be unavailable for new data\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing decision regions ...\n",
      "Time to calculate visualization for 100 samples: 31.20 sec\n"
     ]
    }
   ],
   "source": [
    "n_samples = 100\n",
    "sample_ids = np.random.choice(len(trainset), n_samples)\n",
    "X = np.array([ trainset[i][0].cpu().numpy() for i in sample_ids ])\n",
    "Y = np.array([ trainset[i][1] for i in sample_ids ])\n",
    "\n",
    "t0 = time.time()\n",
    "deepview.add_samples(X, Y)\n",
    "deepview.show()\n",
    "\n",
    "print('Time to calculate visualization for %d samples: %.2f sec' % (n_samples, time.time() - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add new samples to the visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance calculation 20.00 %\n",
      "Distance calculation 40.00 %\n",
      "Distance calculation 60.00 %\n",
      "Distance calculation 80.00 %\n",
      "Distance calculation 100.00 %\n",
      "Embedding samples ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luca/Downloads/umap/umap/umap_.py:1428: UserWarning: Using precomputed metric; transform will be unavailable for new data\n",
      "  warn(\"Using precomputed metric; transform will be unavailable for new data\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing decision regions ...\n",
      "Time to add  100 samples to visualization: 66.89 sec\n"
     ]
    }
   ],
   "source": [
    "n_new = 100\n",
    "\n",
    "sample_ids = np.random.choice(len(trainset), n_new)\n",
    "X = np.array([ trainset[i][0].cpu().numpy() for i in sample_ids ])\n",
    "Y = np.array([ trainset[i][1] for i in sample_ids ])\n",
    "\n",
    "t0 = time.time()\n",
    "deepview.add_samples(X, Y)\n",
    "deepview.show()\n",
    "\n",
    "print('Time to add %d samples to visualization: %.2f sec' % (n_samples, time.time() - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare performance\n",
    "\n",
    "Adding samples may be a bit more time consuming, then just running DeepView on the desired amount of samples to be visualized. This is because the decision boundaries must be calculated twice with a similar time complexity. However, the step of adding 100 samples to 100 existing samples takes less time then computing it from scratch for 200 samples. This is because distances were already computed for half of the samples and can be reused.\n",
    "\n",
    "| <p align=\"left\">Szenario | Time |\n",
    "| -------- | ---- |\n",
    "| <p align=\"left\">From scratch for 100 samples | 31.20 sec |\n",
    "| <p align=\"left\">Adding 100 samples (100 already added) | 66.89 sec |\n",
    "| <p align=\"left\">From scratch for 200 samples | 71.16 sec |\n",
    "| <p align=\"left\">200 samples when adding 100 samples in two steps | 98.19 sec |\n",
    "\n",
    "> Choose the usage according to your needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance calculation 20.00 %\n",
      "Distance calculation 40.00 %\n",
      "Distance calculation 60.00 %\n",
      "Distance calculation 80.00 %\n",
      "Distance calculation 100.00 %\n",
      "Embedding samples ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luca/Downloads/umap/umap/umap_.py:1428: UserWarning: Using precomputed metric; transform will be unavailable for new data\n",
      "  warn(\"Using precomputed metric; transform will be unavailable for new data\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing decision regions ...\n",
      "Time to calculate visualization for 200 samples: 71.16 sec\n"
     ]
    }
   ],
   "source": [
    "deepview = DeepView(pred_wrapper, classes, max_samples, batch_size, \n",
    "                    img_size, img_channels, n, lam, resolution, cmap)\n",
    "\n",
    "n_samples = 200\n",
    "sample_ids = np.random.choice(len(trainset), n_samples)\n",
    "X = np.array([ trainset[i][0].cpu().numpy() for i in sample_ids ])\n",
    "Y = np.array([ trainset[i][1] for i in sample_ids ])\n",
    "\n",
    "t0 = time.time()\n",
    "deepview.add_samples(X, Y)\n",
    "deepview.show()\n",
    "\n",
    "print('Time to calculate visualization for %d samples: %.2f sec' % (n_samples, time.time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2.0",
   "language": "python",
   "name": "tf2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
